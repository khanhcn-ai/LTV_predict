# D·ª± √Ån D·ª± ƒêo√°n LTV t·ª´ D4-D60

## T·ªïng Quan D·ª± √Ån

D·ª± √°n n√†y x√¢y d·ª±ng h·ªá th·ªëng machine learning ƒë·ªÉ d·ª± ƒëo√°n **Lifetime Value (LTV)** c·ªßa ng∆∞·ªùi d√πng t·ª´ ng√†y th·ª© 4 ƒë·∫øn ng√†y th·ª© 60 . H·ªá th·ªëng s·ª≠ d·ª•ng ki·∫øn tr√∫c **ensemble stacking** v·ªõi nhi·ªÅu model h·ªçc m√°y ƒë·ªÉ ƒë·∫°t ƒë·ªô ch√≠nh x√°c cao.

## üéØ M·ª•c Ti√™u

- **D·ª± ƒëo√°n LTV** cho 57 ng√†y li√™n ti·∫øp (D4 ‚Üí D60)
- **T·ªëi ∆∞u h√≥a ƒë·ªô ch√≠nh x√°c** s·ª≠ d·ª•ng ensemble stacking
- **X·ª≠ l√Ω d·ªØ li·ªáu time-series** gaming v·ªõi ƒë·∫∑c th√π ph·ª©c t·∫°p
- **T·ª± ƒë·ªông h√≥a pipeline** training v√† evaluation

## üìä Ki·∫øn Tr√∫c H·ªá Th·ªëng

### 1. **Base Models (Level 1)**
- **XGBoost Regressor**: Gradient boosting v·ªõi objective='reg:absoluteerror'
- **LightGBM Regressor**: Light gradient boosting v·ªõi regression_l1
- **CatBoost Regressor**: Categorical features handling v·ªõi MAE loss

### 2. **Meta Model (Level 2)**
- **XGBoost Meta**: K·∫øt h·ª£p predictions t·ª´ 3 base models
- Input: `[pred_xgb, pred_lgbm, pred_cat]`

### 3. **Residual Model (Level 3)**
- **XGBoost Residual**: Hi·ªáu ch·ªânh predictions cu·ªëi c√πng
- Input: `[base_preds, meta_stats, meta_pred]`
- Features: `mean, std, range` c·ªßa base predictions + meta prediction

### 4. **Feature Engineering Pipeline**
```
Raw Data ‚Üí Base Features ‚Üí Polynomial Features ‚Üí Power Transform ‚Üí Ensemble Input
```

## üìÅ C·∫•u Tr√∫c Th∆∞ M·ª•c

```
‚îú‚îÄ‚îÄ data/                           # D·ªØ li·ªáu ƒë·∫ßu v√†o
‚îÇ   ‚îú‚îÄ‚îÄ 2025-01-01_2025-03-02_puzzle_com.twisted.rope.tangle.csv
‚îÇ   ‚îú‚îÄ‚îÄ 2025-08-01_2025-09-30_com.wool.puzzle.game3d.csv
‚îÇ   ‚îî‚îÄ‚îÄ raw_2025-04-01_2025-05-31_puzzle_com.twisted.rope.tangle.csv
‚îú‚îÄ‚îÄ model/                          # Trained models (.joblib)
‚îÇ   ‚îú‚îÄ‚îÄ ltv_d4_stack_pipeline.joblib
‚îÇ   ‚îú‚îÄ‚îÄ ltv_d5_stack_pipeline.joblib
‚îÇ   ‚îî‚îÄ‚îÄ ... (57 models for D4-D60)
‚îú‚îÄ‚îÄ modeling/                       # Core modeling scripts
‚îÇ   ‚îú‚îÄ‚îÄ build_base_model.py         # XGBoost, LightGBM, CatBoost
‚îÇ   ‚îú‚îÄ‚îÄ build_stack_model.py        # Meta model creation
‚îÇ   ‚îú‚îÄ‚îÄ build_residual_model.py     # Residual correction
‚îÇ   ‚îú‚îÄ‚îÄ train_model_per_day.py      # Daily training pipeline
‚îÇ   ‚îú‚îÄ‚îÄ optuna_tunning.py           # Hyperparameter optimization
‚îÇ   ‚îî‚îÄ‚îÄ calculate_nae.py           # Evaluation metrics
‚îú‚îÄ‚îÄ feature/                        # Feature engineering
‚îÇ   ‚îú‚îÄ‚îÄ feature_adding.py          # Feature creation & polynomial transforms
‚îÇ   ‚îú‚îÄ‚îÄ get_oof_predict.py         # Out-of-fold predictions
‚îÇ   ‚îî‚îÄ‚îÄ mixup_train_data.py        # Data augmentation
‚îú‚îÄ‚îÄ ans/                           # K·∫øt qu·∫£ evaluation
‚îÇ   ‚îú‚îÄ‚îÄ ket_qua_danh_gia.txt       # NAE scores on test set
‚îÇ   ‚îî‚îÄ‚îÄ NAE_ans_ensemble.txt       # NAE scores with ensemble
‚îú‚îÄ‚îÄ notebook/                      # Jupyter notebooks
‚îÇ   ‚îú‚îÄ‚îÄ draw1.ipynb               # Performance visualization
‚îÇ   ‚îî‚îÄ‚îÄ XGBoost_ensemble.ipynb    # Model analysis
‚îî‚îÄ‚îÄ train_model_total.py          # Main training script
```

## üîß Feature Engineering

### Base Features (D0-D3)
```python
# Core metrics
roas_d0, roas_d1, roas2, roas_d3
cumulative_revenue_d0-3
daily_revenue_d0-3
unique_users_d0-3
ltv_d0-3
cost
```

### Engineered Features
- **Aggregations**: `mean, std` c·ªßa LTV, ROAS, Revenue
- **Growth Rates**: `ltv_growth, roas_trend, revenue_acceleration`
- **Ratios**: `ltv_roas_ratio, ARPU_d0-3, retention_d1-3`
- **Advanced Metrics**: 
  - `LTV_CAC = ltv_d3 / (cost/cumulative_users_d3)`
  - `Payback_Velocity = (cumulative_revenue_d3/cost) / 4`
  - `ARPU_trend = ARPU_d3 - ARPU_d0`

### Polynomial Features
- **Degree 2** polynomial expansion tr√™n key features
- **Selected features**: `ltv_mean, cost, LTV_CAC, Payback_Velocity`
- **Automatic feature selection** lo·∫°i b·ªè redundant features

### Data Transformations
- **Power Transform** (Yeo-Johnson) cho c·∫£ X v√† y
- **Out-of-fold predictions** t·ª´ ElasticNet models
- **Mixup augmentation** cho training data

## üöÄ C√°ch Ch·∫°y H·ªá Th·ªëng

### 1. Training Models
```bash
# Train to√†n b·ªô pipeline D4-D60
python train_model_total.py

# Train model cho ng√†y c·ª• th·ªÉ
python -c "from modeling.train_model_per_day import *; build_model_per_day(30)"
```

### 2. Evaluation
```bash
# ƒê√°nh gi√° performance tr√™n test set
python evaluate.py

# Ki·ªÉm tra single prediction
python check.py
```

### 3. Visualization
```bash
# Ch·∫°y notebook ƒë·ªÉ v·∫Ω performance charts
jupyter notebook notebook/draw1.ipynb
```

## üìà K·∫øt Qu·∫£ Hi·ªáu Su·∫•t

### NAE (Normalized Absolute Error) Performance

#### Tr√™n Test Set (Final Ensemble)
| Day Range | NAE (%) | Performance |
|-----------|---------|-------------|
| D4-D10    | 2.99-5.50| ‚≠ê‚≠ê‚≠ê Excellent |
| D11-D20   | 5.84-7.19| ‚≠ê‚≠ê Very Good |
| D21-D30   | 6.12-7.50| ‚≠ê‚≠ê Very Good |
| D31-D40   | 6.39-8.68| ‚≠ê‚≠ê Good |
| D41-D50   | 7.24-9.22| ‚≠ê Good |
| D51-D60   | 7.84-9.22| ‚≠ê Good |

#### Key Insights
- **NAE trung b√¨nh**: ~7.2% across all days
- **Best performance**: D4 v·ªõi 2.99% NAE
- **Stability**: Relative stability sau D15
- **Trend**: Gradual increase in error over time (expected)

### Model Architecture Benefits
- **Stacking**: C·∫£i thi·ªán ~15-20% so v·ªõi single models
- **Residual correction**: Gi·∫£m ~5% NAE
- **Feature engineering**: C·∫£i thi·ªán ~25% so v·ªõi raw features
- **Hyperparameter tuning**: C·∫£i thi·ªán ~10% v·ªõi Optuna

## üéØ Model Pipeline Details

### 1. Training Pipeline Per Day
```python
def build_model_per_day(input_day):
    # 1. Data preparation
    df = get_data_for_train(input_day)
    X, y = feature_for_X(df)
    
    # 2. Train/validation split
    X_train, X_test, y_train, y_test = train_test_split()
    
    # 3. Feature engineering
    X_train, poly_transformer = feature_for_X(X_train)
    X_train = apply_mixup_train_data(X_train, y_train)
    
    # 4. Power transformation
    X_train_transformed = power_X.fit_transform(X_train)
    y_train_transformed = power_y.fit_transform(y_train)
    
    # 5. Base models training
    xgb_model = build_xgboost_model(d_train, X_train_transformed, y_train_transformed)
    lgbm_model = build_lightgbm_model(d_train, X_train_transformed, y_train_transformed)
    cat_model = build_catboost_model(X_train_transformed, y_train_transformed)
    
    # 6. Meta model training
    meta_model = build_stacking_model(xgb_params, lgbm_params, cat_params, ...)
    
    # 7. Residual model training
    res_model = build_res_model(meta_params, ...)
    
    # 8. Save artifacts
    joblib.dump(artifacts, f"model/ltv_d{input_day}_stack_pipeline.joblib")
```

### 2. Prediction Pipeline
```python
def load_and_predict_ensemble(input_data_df, input_day):
    # 1. Load model artifacts
    artifacts = load_model(f"ltv_d{input_day}_stack_pipeline.joblib")
    
    # 2. Feature engineering
    df = get_data_for_infer(input_data_df, input_day, poly_transformer)
    
    # 3. Transform data
    X_transformed = power_X.transform(X)
    
    # 4. OOF predictions (ElasticNet ensemble)
    oof_pred = ensemble_elasticnet_predict(X_transformed, oof_models)
    X_transformed['OOF_ElasticNet'] = oof_pred
    
    # 5. Base model predictions
    pred_xgb = xgb_model.predict(X_transformed)
    pred_lgbm = lgbm_model.predict(X_transformed)
    pred_cat = cat_model.predict(X_transformed)
    
    # 6. Meta model prediction
    X_meta = np.column_stack([pred_xgb, pred_lgbm, pred_cat])
    pred_meta = meta_model.predict(X_meta)
    
    # 7. Residual model correction
    meta_stats = calculate_meta_statistics(X_meta)
    X_res = np.column_stack([pred_xgb, pred_lgbm, pred_cat, meta_stats, pred_meta])
    residual_correction = residual_model.predict(X_res)
    
    # 8. Final prediction
    final_pred_transformed = pred_meta + residual_correction
    final_prediction = power_y.inverse_transform(final_pred_transformed)
    
    return final_prediction
```

## üîç Dependencies

### Core Libraries
```python
scikit-learn>=1.0.0    # ML algorithms & preprocessing
xgboost>=1.5.0         # Gradient boosting
lightgbm>=3.2.0        # Light gradient boosting  
catboost>=1.0.6        # Categorical features handling
optuna>=2.10.0         # Hyperparameter optimization
joblib>=1.1.0          # Model serialization
pandas>=1.3.0          # Data manipulation
numpy>=1.21.0          # Numerical computing
matplotlib>=3.4.0      # Visualization
seaborn>=0.11.0        # Statistical visualization
```

### Key Features
- **GPU Support**: CatBoost v·ªõi `task_type='GPU'`
- **Memory Efficient**: DMatrix cho XGBoost, Dataset cho LightGBM
- **Parallel Processing**: Optuna multi-threading
- **Cross-validation**: 5-fold KFold v·ªõi shuffle

## üìä Data Schema

### Input Data Format
D·ªØ li·ªáu ƒë·∫ßu v√†o ch·ª©a c√°c c·ªôt sau:

#### Basic Information
- `app_id, media_source, campaign, geo, game_type`

#### ROAS Metrics (D0-D60)
- `roas_d0, roas_d1, ..., roas_d60`

#### Revenue Metrics (D0-D60)  
- `cumulative_revenue_d0, ..., cumulative_revenue_d60`
- `daily_revenue_d0, ..., daily_revenue_d60`

#### User Metrics (D0-D60)
- `unique_users_d0, ..., unique_users_d60`

#### LTV Values (D0-D60)
- `ltv_d0, ltv_d1, ..., ltv_d60`

#### Cost Information
- `cost`: Marketing spend

### Output Format
```python
# Model artifacts structure
artifacts = {
    "power_X": PowerTransformer,        # Feature transformer
    "power_y": PowerTransformer,        # Target transformer  
    "feature_list": List[str],         # Feature names
    "poly_transform": PolynomialFeatures, # Polynomial transformer
    "oof_predict_models": List[Dict],   # ElasticNet ensemble
    "base_models": {
        "xgb": XGBRegressor,
        "lgbm": LGBMRegressor, 
        "cat": CatBoostRegressor
    },
    "meta_model": XGBRegressor,         # Meta model
    "residual_model": XGBRegressor      # Residual model
}
```

## üèÜ Highlights & Innovations

### 1. **Multi-Level Stacking**
```python
# 3-level ensemble architecture
Level 1: Base Models (XGB + LGBM + CatBoost)
Level 2: Meta Model (XGB on base predictions)  
Level 3: Residual Model (XGB on meta + statistics)
```

### 2. **Advanced Feature Engineering**
- **Polynomial expansion** v·ªõi automatic feature selection
- **Statistical features** t·ª´ prediction ensembles
- **Time-series aggregations** v√† trend analysis
- **Ratio-based features** cho business insights

### 3. **Robust Data Pipeline**
- **Power transformation** cho skewed distributions
- **Out-of-fold predictions** prevent data leakage  
- **Mixup augmentation** improve generalization
- **Cross-validation** cho reliable evaluation

### 4. **Automated Optimization**
- **Optuna hyperparameter tuning** v·ªõi pruning
- **Multi-objective optimization** (accuracy vs complexity)
- **Early stopping** v√† best trial selection

## üìù Usage Examples

### Basic Prediction
```python
from evaluate import load_and_predict_ensemble
import pandas as pd

# Load new data
df = pd.read_csv("new_gaming_data.csv")

# Predict LTV for D30
nae_score, runtime = load_and_predict_ensemble(df, input_day=30)

print(f"NAE Score: {nae_score}%")
print(f"Runtime: {runtime}s")
```

### Custom Feature Engineering
```python
from feature.feature_adding import feature_for_X

# Apply feature engineering to new data
X_features, poly_transformer = feature_for_X(raw_data)
X_engineered = feature_for_X(new_data, poly_transformer)
```

### Model Inspection
```python
import joblib

# Load trained model
model = joblib.load("model/ltv_d30_stack_pipeline.joblib")

# Inspect model components
print("Base models:", list(model['base_models'].keys()))
print("Feature count:", len(model['feature_list']))
print("Polynomial degree:", model['poly_transform'].degree)
```

## üéÆ Domain Context

D·ª± √°n n√†y ƒë∆∞·ª£c thi·∫øt k·∫ø ƒë·∫∑c bi·ªát cho **mobile gaming industry** v·ªõi c√°c th√°ch th·ª©c:

### Gaming-Specific Challenges
- **High volatility** trong user behavior
- **Seasonal patterns** v√† event-driven spikes
- **Geographic differences** trong monetization
- **Platform variations** (iOS vs Android)

### Business Applications
- **User acquisition optimization** d·ª±a tr√™n predicted LTV
- **Campaign performance prediction** tr∆∞·ªõc khi spend budget
- **Market expansion decisions** cho new geographies
- **ROI forecasting** cho marketing activities

## üîÆ Future Improvements

### Short Term
- [ ] **Deep learning models** (Neural Networks, LSTM)
- [ ] **Time series forecasting** (Prophet, ARIMA)
- [ ] **Real-time prediction API** v·ªõi FastAPI
- [ ] **A/B testing framework** cho model comparison

### Long Term
- [ ] **Multi-game prediction** (cross-game learning)
- [ ] **Federated learning** cho privacy-preserving models
- [ ] **Causal inference** methods cho better feature selection
- [ ] **Bayesian optimization** cho hyperparameter tuning

## üë• Contributing

ƒê·ªÉ contribute v√†o d·ª± √°n:

1. Fork repository
2. T·∫°o feature branch: `git checkout -b feature/new-feature`
3. Commit changes: `git commit -m "Add new feature"`
4. Push to branch: `git push origin feature/new-feature`
5. Submit Pull Request

### Development Setup
```bash
# Clone repository
git clone <repository-url>
cd ltv-prediction-project

# Install dependencies
pip install -r requirements.txt

# Run tests
python -m pytest tests/

# Lint code
flake8 .
black .
```

## üìÑ License

D·ª± √°n ƒë∆∞·ª£c licensed under MIT License - xem file [LICENSE](LICENSE) ƒë·ªÉ bi·∫øt chi ti·∫øt.

## üôè Acknowledgments

- **Mobile Gaming Data**: Provided by gaming analytics platform
- **ML Framework**: Scikit-learn, XGBoost, LightGBM, CatBoost teams
- **Optimization**: Optuna team for hyperparameter tuning
- **Community**: Open source ML community for tools v√† resources

---

## üìû Contact

- **Author**: Machine Learning Team
- **Email**: ml-team@company.com
- **Documentation**: [Project Wiki](wiki-url)
- **Issues**: [GitHub Issues](issues-url)

**L∆∞u √Ω**: D·ª± √°n n√†y ch·ª©a proprietary data v√† algorithms. Vui l√≤ng tu√¢n th·ªß data privacy policies v√† intellectual property rights.
